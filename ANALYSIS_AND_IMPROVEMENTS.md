# Vanelia Pipeline ìƒì„¸ ë¶„ì„ ë° ê°œì„ ì•ˆ

## ğŸ“Š í˜„ì¬ íŒŒì´í”„ë¼ì¸ í‰ê°€

### Architecture Overview
```
Video â†’ Dust3R (Camera) â†’ Blender (Render) â†’ IC-Light (Relight) â†’ Final Video
```

---

## ğŸ” Module-by-Module ë¶„ì„

### 1ï¸âƒ£ Module A: Dust3R Camera Extraction

#### âœ… ì¥ì 
- ë¹ ë¥¸ ì¶”ë¡  ì†ë„ (~0.5s/frame)
- Monocular videoì—ì„œ ì‘ë™
- Point cloud ë™ì‹œ ìƒì„±

#### âŒ ë¬¸ì œì  ë° ê°œì„ ì•ˆ

**ë¬¸ì œ 1: ì¹´ë©”ë¼ í¬ì¦ˆ ì •í™•ë„ ë¶€ì¡±**
- Dust3Rì€ relative poseë§Œ ì •í™•, absolute scale ë¶€ì •í™•
- ë¹ ë¥¸ ì¹´ë©”ë¼ ì›€ì§ì„ì—ì„œ drift ë°œìƒ
- SfM ê¸°ë°˜ ë°©ë²• ëŒ€ë¹„ ì •ë°€ë„ ë‚®ìŒ

**ê°œì„ ì•ˆ A1: MASt3Rë¡œ ì—…ê·¸ë ˆì´ë“œ**
```python
# MASt3R (2024): Dust3Rì˜ ê°œì„  ë²„ì „
# https://github.com/naver/mast3r
from mast3r.model import AsymmetricMASt3R

# ì¥ì :
# - Matching + Stereo í†µí•©
# - ë” ì •í™•í•œ depth estimation
# - Better scale consistency
```

**ê°œì„ ì•ˆ A2: COLMAP + SuperPoint/SuperGlue í•˜ì´ë¸Œë¦¬ë“œ**
```python
# ë” ì •í™•í•œ SfM pipeline
# 1. SuperPointë¡œ feature extraction
# 2. SuperGlueë¡œ matching
# 3. COLMAPë¡œ bundle adjustment
# 4. Dust3Rë¡œ dense reconstruction

# ì¥ì : Production-level ì •í™•ë„
# ë‹¨ì : ëŠë¦¼ (~5-10s/frame)
```

**ê°œì„ ì•ˆ A3: DROID-SLAM (ì‹¤ì‹œê°„ SLAM)**
```python
# https://github.com/princeton-vl/DROID-SLAM
# ì¥ì :
# - Real-time tracking
# - Loop closure detection
# - Better scale estimation
# ë‹¨ì : GPU ë©”ëª¨ë¦¬ ë§ì´ ì‚¬ìš©
```

---

### 2ï¸âƒ£ Module B: Blender Rendering

#### âœ… ì¥ì 
- ë¬¼ë¦¬ ê¸°ë°˜ ë Œë”ë§ (Cycles)
- GLB ì¬ì§ˆ ì§€ì›
- Shadow catcher

#### âŒ ë¬¸ì œì  ë° ê°œì„ ì•ˆ

**ë¬¸ì œ 1: ì¬ì§ˆì´ ì—¬ì „íˆ ì–´ë‘¡ê±°ë‚˜ ë¶€ì •í™•**
```python
# í˜„ì¬ ì½”ë“œ (blender_render.py:95-99)
bpy.ops.import_scene.gltf(
    filepath=glb_path,
    import_shading='FLAT',  # âŒ ë¬¸ì œ!
    merge_vertices=False
)
```

**ê°œì„ ì•ˆ B1: ì¬ì§ˆ Import ìˆ˜ì •**
```python
# FLAT ëŒ€ì‹  NODES ì‚¬ìš©
bpy.ops.import_scene.gltf(
    filepath=glb_path,
    import_shading='NORMALS',  # âœ… PBR ë…¸ë“œ ìœ ì§€
    merge_vertices=False,
    bone_heuristic='TEMPERANCE'
)

# Import í›„ ì¬ì§ˆ ê²€ì¦ ë° ìˆ˜ì •
for obj in imported_objects:
    if obj.type == 'MESH':
        for slot in obj.material_slots:
            mat = slot.material
            if mat and mat.use_nodes:
                # PBR ë…¸ë“œ í™•ì¸
                nodes = mat.node_tree.nodes
                bsdf = nodes.get('Principled BSDF')
                if bsdf:
                    # Base Colorê°€ ê²€ì€ìƒ‰ì´ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ
                    if bsdf.inputs['Base Color'].default_value[0] < 0.01:
                        bsdf.inputs['Base Color'].default_value = (0.8, 0.8, 0.8, 1.0)
```

**ë¬¸ì œ 2: í™˜ê²½ ì¡°ëª…ì´ ë¶€ìì—°ìŠ¤ëŸ¬ì›€**
```python
# í˜„ì¬: procedural skyë§Œ ì‚¬ìš© (blender_render.py:258-262)
node_sky.sky_type = 'NISHITA'  # âŒ ì‹¤ì œ ë°°ê²½ê³¼ ì•ˆ ë§ìŒ
```

**ê°œì„ ì•ˆ B2: Background Imageì—ì„œ HDRI ì¶”ì¶œ**
```python
# ë°°ê²½ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ì„œ ì¡°ëª… ì¶”ì •
def estimate_lighting_from_background(bg_image_path):
    """
    ë°°ê²½ ì´ë¯¸ì§€ì—ì„œ ì¡°ëª… ë°©í–¥/ìƒ‰ìƒ ì¶”ì •
    """
    import cv2
    from scipy.ndimage import gaussian_filter

    img = cv2.imread(bg_image_path)
    img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)

    # ë°ì€ ì˜ì—­ = ê´‘ì›
    brightness = img_hsv[:, :, 2]
    smooth_bright = gaussian_filter(brightness, sigma=20)

    # ê°€ì¥ ë°ì€ ì˜ì—­ ì°¾ê¸°
    max_loc = np.unravel_index(smooth_bright.argmax(), smooth_bright.shape)

    # ê°ë„ ê³„ì‚° (ì´ë¯¸ì§€ ì¤‘ì‹¬ ê¸°ì¤€)
    h, w = img.shape[:2]
    dx = max_loc[1] - w/2
    dy = max_loc[0] - h/2

    sun_azimuth = np.arctan2(dy, dx)
    sun_elevation = np.radians(45)  # ê¸°ë³¸ê°’, ê¹Šì´ ì •ë³´ë¡œ ê°œì„  ê°€ëŠ¥

    # ìƒ‰ì˜¨ë„ ì¶”ì •
    bright_region = img[smooth_bright > np.percentile(smooth_bright, 90)]
    avg_color = np.mean(bright_region, axis=0) / 255.0

    return {
        'sun_azimuth': sun_azimuth,
        'sun_elevation': sun_elevation,
        'color': avg_color
    }

# Blenderì—ì„œ ì ìš©
lighting = estimate_lighting_from_background(background_frames[0])
node_sky.sun_rotation = lighting['sun_azimuth']
node_sky.sun_elevation = lighting['sun_elevation']
```

**ë¬¸ì œ 3: Shadow Catcherê°€ ì§€ë©´ê³¼ ì•ˆ ë§ìŒ**
```python
# í˜„ì¬: í•­ìƒ Z=0 í‰ë©´ (blender_render.py:363)
renderer.create_shadow_catcher(size=20.0)  # âŒ ì§€ë©´ ê°ë„ ë¬´ì‹œ
```

**ê°œì„ ì•ˆ B3: Ground Planeì— ë§ì¶˜ Shadow Catcher**
```python
def create_aligned_shadow_catcher(self, ground_plane: dict, size: float = 20.0):
    """
    ê²€ì¶œëœ ì§€ë©´ í‰ë©´ì— ì •ë ¬ëœ Shadow Catcher ìƒì„±
    """
    # í‰ë©´ ìƒì„±
    bpy.ops.mesh.primitive_plane_add(size=size, location=(0, 0, 0))
    plane = bpy.context.active_object

    if ground_plane:
        # ì§€ë©´ normalì— ë§ì¶° íšŒì „
        normal = np.array(ground_plane['normal'])

        # Zì¶•ì„ normal ë°©í–¥ìœ¼ë¡œ íšŒì „í•˜ëŠ” quaternion ê³„ì‚°
        from mathutils import Vector, Matrix
        z_axis = Vector((0, 0, 1))
        normal_vec = Vector(normal)

        rotation_matrix = z_axis.rotation_difference(normal_vec).to_matrix().to_4x4()
        plane.matrix_world = rotation_matrix

        # ì§€ë©´ ìœ„ì¹˜ë¡œ ì´ë™
        A, B, C, D = ground_plane['A'], ground_plane['B'], ground_plane['C'], ground_plane['D']
        # ì›ì ì—ì„œ í‰ë©´ê¹Œì§€ ê±°ë¦¬
        if abs(C) > 1e-6:
            z_offset = -D / C
            plane.location.z = z_offset

    # Shadow catcher ì„¤ì •
    plane.is_shadow_catcher = True
    # ...
```

**ë¬¸ì œ 4: GPU ìµœì í™” ë¶€ì¡±**
```python
# í˜„ì¬ (blender_render.py:59)
scene.cycles.device = 'GPU'  # âŒ ì–´ë–¤ GPUì¸ì§€ ëª…ì‹œ ì•ˆí•¨
```

**ê°œì„ ì•ˆ B4: GPU ì„¤ì • ìµœì í™”**
```python
def optimize_gpu_settings(self):
    """
    CUDA/Optix ìµœì í™”
    """
    import bpy

    # GPU í™œì„±í™”
    prefs = bpy.context.preferences.addons['cycles'].preferences
    prefs.compute_device_type = 'CUDA'  # ë˜ëŠ” 'OPTIX' for RTX

    # ëª¨ë“  GPU í™œì„±í™”
    for device in prefs.devices:
        device.use = True

    scene = bpy.context.scene
    scene.cycles.device = 'GPU'

    # Optix denoiser (RTX GPU ì „ìš©, í›¨ì”¬ ë¹ ë¦„)
    scene.cycles.denoiser = 'OPTIX'

    # Tile í¬ê¸° ìµœì í™”
    scene.render.tile_x = 256
    scene.render.tile_y = 256

    # ì„±ëŠ¥ ì„¤ì •
    scene.cycles.use_adaptive_sampling = True
    scene.cycles.adaptive_threshold = 0.01
    scene.cycles.samples = 64  # 128ì—ì„œ ë‚®ì¶°ë„ Optix denoiserë¡œ ê¹¨ë—
```

---

### 3ï¸âƒ£ Module C: IC-Light Compositing

#### âœ… ì¥ì 
- Relighting í’ˆì§ˆ ì¢‹ìŒ
- Fixed seedë¡œ ì¼ê´€ì„± ìœ ì§€

#### âŒ ë¬¸ì œì  ë° ê°œì„ ì•ˆ

**ë¬¸ì œ 1: IC-Light SD1.5 ê¸°ë°˜ (êµ¬í˜•)**
```python
# í˜„ì¬ (iclight_compositor.py:36)
model_id = "lllyasviel/ic-light-sd15-fc"  # âŒ SD1.5 (2022)
```

**ê°œì„ ì•ˆ C1: SDXL ê¸°ë°˜ ëª¨ë¸ë¡œ ì—…ê·¸ë ˆì´ë“œ**
```python
# Option 1: IC-Light SDXL (ë” ë‚˜ì€ í’ˆì§ˆ)
# https://huggingface.co/lllyasviel/ic-light-sdxl

# Option 2: ControlNet + SDXL
from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel

controlnet = ControlNetModel.from_pretrained(
    "diffusers/controlnet-canny-sdxl-1.0",
    torch_dtype=torch.float16
)

pipe = StableDiffusionXLControlNetPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    controlnet=controlnet,
    torch_dtype=torch.float16
).to("cuda")

# ì¥ì :
# - ë” ë†’ì€ í•´ìƒë„ (1024x1024)
# - ë” ë‚˜ì€ ë””í…Œì¼
# - ìµœì‹  ì•„í‚¤í…ì²˜
```

**ë¬¸ì œ 2: Frame-by-frame ì²˜ë¦¬ (Temporal Consistency ì•½í•¨)**
```python
# í˜„ì¬: ê° í”„ë ˆì„ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬
for idx, (render_path, bg_path) in enumerate(zip(render_frames, bg_frames)):
    output_img, current_latent = self.process_frame(...)
```

**ê°œì„ ì•ˆ C2: Video Diffusion Model ì‚¬ìš©**
```python
# Stable Video Diffusion (SVD) for temporal consistency
from diffusers import StableVideoDiffusionPipeline

pipe = StableVideoDiffusionPipeline.from_pretrained(
    "stabilityai/stable-video-diffusion-img2vid-xt",
    torch_dtype=torch.float16
).to("cuda")

# ì¥ì :
# - Native video consistency
# - ìë™ìœ¼ë¡œ temporal coherence ìœ ì§€
# - Flickering ê±°ì˜ ì—†ìŒ

# ë‹¨ì :
# - ëŠë¦¼ (ì „ì²´ ë¹„ë””ì˜¤ í•œë²ˆì— ì²˜ë¦¬)
# - ë©”ëª¨ë¦¬ ë§ì´ ì‚¬ìš©
```

**ê°œì„ ì•ˆ C3: TokenFlow (ë” ë‚˜ì€ consistency)**
```python
# https://github.com/omerbt/TokenFlow
# - Cross-frame attentionìœ¼ë¡œ ì¼ê´€ì„± ìœ ì§€
# - Existing diffusion modelì— í”ŒëŸ¬ê·¸ì¸ ê°€ëŠ¥

from tokenflow import TokenFlow

tokenflow = TokenFlow(pipe, num_frames=len(frames))
output_frames = tokenflow.generate(
    frames=composite_frames,
    prompt=prompt,
    strength=0.25,
    seed=seed
)
```

**ë¬¸ì œ 3: Alpha Blendingë§Œ ì‚¬ìš© (Depth ë¬´ì‹œ)**
```python
# í˜„ì¬ (iclight_compositor.py:68)
blended = (fg_rgb * alpha + background * (1 - alpha))  # âŒ Depth ê³ ë ¤ ì•ˆí•¨
```

**ê°œì„ ì•ˆ C4: Depth-Aware Compositing**
```python
def depth_aware_composite(self, fg_rgba, bg_rgb, fg_depth, bg_depth):
    """
    ê¹Šì´ ì •ë³´ë¥¼ í™œìš©í•œ í•©ì„±
    """
    # Foreground depth < Background depthì¸ í”½ì…€ë§Œ í•©ì„±
    fg_rgb = fg_rgba[:, :, :3]
    alpha = fg_rgba[:, :, 3:4] / 255.0

    # Depth mask (fgê°€ bgë³´ë‹¤ ì•ì— ìˆëŠ” ê³³ë§Œ)
    depth_mask = (fg_depth < bg_depth).astype(np.float32)

    # Alphaì™€ depth mask ê²°í•©
    final_alpha = alpha * depth_mask[:, :, np.newaxis]

    # Compositing
    blended = (fg_rgb * final_alpha +
               bg_rgb * (1 - final_alpha)).astype(np.uint8)

    return blended

# Background depth ì¶”ì •
from transformers import pipeline
depth_estimator = pipeline("depth-estimation", model="Intel/dpt-large")

bg_depth = depth_estimator(bg_img)['depth']
```

---

## ğŸš€ ëŒ€ì•ˆ íŒŒì´í”„ë¼ì¸ ì œì•ˆ

### Option 1: NeRF/3DGS ê¸°ë°˜ (ìµœê³  í’ˆì§ˆ)

```
Video â†’ COLMAP â†’ NeRF/3DGS Training â†’ Insert 3D Object â†’ Novel View Rendering
```

**ì¥ì :**
- ì™„ë²½í•œ ì¡°ëª… ì¼ì¹˜
- ì •í™•í•œ geometry
- Photo-realistic ê²°ê³¼

**ë‹¨ì :**
- ë§¤ìš° ëŠë¦¼ (NeRF training ìˆ˜ ì‹œê°„)
- GPU ë©”ëª¨ë¦¬ ë§ì´ í•„ìš”

**êµ¬í˜„:**
```python
# 1. Nerfstudioë¡œ ì”¬ ì¬êµ¬ì„±
ns-train nerfacto --data video.mp4

# 2. 3D ê°ì²´ ì‚½ì… (Blender)
# 3. NeRF rendering with object

# ì°¸ê³ : https://github.com/nerfstudio-project/nerfstudio
```

### Option 2: Depth-ControlNet (ë¹ ë¥´ê³  íš¨ê³¼ì )

```
Video â†’ Depth Estimation â†’ Blender Render â†’ ControlNet Refinement
```

**ì¥ì :**
- Depthë¡œ geometry ì œì•½
- ë¹ ë¦„ (~1s/frame)
- ì¢‹ì€ í’ˆì§ˆ

**êµ¬í˜„:**
```python
# 1. ZoeDepthë¡œ depth map ì¶”ì¶œ
from zoedepth.models.builder import build_model
model = build_model("zoedepth_nk")

# 2. Blender ë Œë”ë§ (depth mapë„ ê°™ì´)
# 3. ControlNet-Depthë¡œ refinement
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel

controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/control_v11f1p_sd15_depth"
)
```

### Option 3: Blender Only (ë‹¨ìˆœí•˜ì§€ë§Œ ë¹ ë¦„)

```
Video â†’ COLMAP â†’ Blender Compositing (No AI)
```

**ì¥ì :**
- ë§¤ìš° ë¹ ë¦„
- Predictable
- No flickering

**ë‹¨ì :**
- ì¡°ëª… ì¼ì¹˜ ìˆ˜ë™ ì‘ì—…
- AI refinement ì—†ìŒ

---

## ğŸ¯ ìµœì¢… ê¶Œì¥ ì‚¬í•­

### ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ê°œì„  (High Impact, Low Effort)

**1. Blender ì¬ì§ˆ ìˆ˜ì •**
```python
# blender_render.py:95
import_shading='NORMALS'  # FLAT â†’ NORMALS
```

**2. GPU ìµœì í™”**
```python
scene.cycles.denoiser = 'OPTIX'
scene.cycles.samples = 64  # 128 â†’ 64
```

**3. Background-based ì¡°ëª… ì¶”ì •**
```python
lighting = estimate_lighting_from_background(bg_frames[0])
# ìœ„ì—ì„œ ì œì‹œí•œ í•¨ìˆ˜ ì¶”ê°€
```

**4. IC-Light SDXL ì—…ê·¸ë ˆì´ë“œ**
```python
model_id = "lllyasviel/ic-light-sdxl"  # SD1.5 â†’ SDXL
```

### ì¤‘ê¸° ê°œì„  (Better Quality)

**1. MASt3Rë¡œ ì¹´ë©”ë¼ ì¶”ì • êµì²´**
```bash
pip install mast3r
```

**2. ControlNet-Depth ì¶”ê°€**
- ZoeDepth í†µí•©
- Depth-guided refinement

**3. Aligned Shadow Catcher**
- Ground planeì— ì •ë ¬

### ì¥ê¸° ê°œì„  (Production Quality)

**1. NeRF/3DGS íŒŒì´í”„ë¼ì¸ êµ¬ì¶•**
- NerfStudio í†µí•©
- ì™„ë²½í•œ ì¡°ëª… ì¼ì¹˜

**2. Video Diffusion Model**
- SVD ë˜ëŠ” TokenFlow
- Perfect temporal consistency

---

## ğŸ“ ì½”ë“œ í’ˆì§ˆ ê°œì„ 

### ì—ëŸ¬ ì²˜ë¦¬ ë¶€ì¡±

**í˜„ì¬:**
```python
# dust3r_camera_extraction.py:48
self.model = AsymmetricCroCo3DStereo.from_pretrained(model_path).to(device)
# âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ì²˜ë¦¬ ì—†ìŒ
```

**ê°œì„ :**
```python
try:
    self.model = AsymmetricCroCo3DStereo.from_pretrained(model_path).to(device)
except Exception as e:
    print(f"ERROR: Failed to load Dust3R model: {e}")
    print("Trying to download...")
    # Fallback to download
    raise
```

### ë©”ëª¨ë¦¬ ê´€ë¦¬

**í˜„ì¬:**
```python
# iclight_compositor.py:218
# ëª¨ë“  latentë¥¼ ë©”ëª¨ë¦¬ì— ìœ ì§€
previous_latent = current_latent  # âŒ ë©”ëª¨ë¦¬ ëˆ„ì 
```

**ê°œì„ :**
```python
# Latentë¥¼ CPUë¡œ ì´ë™
previous_latent = current_latent.cpu()
torch.cuda.empty_cache()
```

### ì§„í–‰ë¥  í‘œì‹œ ê°œì„ 

**ê°œì„ :**
```python
from tqdm import tqdm

for idx in tqdm(range(len(frames)), desc="IC-Light Processing"):
    # ...
```

---

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (ì˜ˆìƒ)

| Pipeline | Speed (per frame) | Quality | Memory |
|----------|------------------|---------|---------|
| **í˜„ì¬** | ~4-7s | â­â­â­ | 12GB |
| **ê°œì„  (ì¦‰ì‹œ)** | ~2-3s | â­â­â­â­ | 10GB |
| **ControlNet-Depth** | ~3-4s | â­â­â­â­ | 14GB |
| **SVD** | ~10s | â­â­â­â­â­ | 24GB |
| **NeRF/3DGS** | ~60s+ | â­â­â­â­â­ | 40GB+ |

---

## ğŸ”§ ë‹¤ìŒ ë‹¨ê³„

1. âœ… **ì¦‰ì‹œ ê°œì„  ì ìš©** (1-2ì‹œê°„)
   - Blender ì¬ì§ˆ ìˆ˜ì •
   - GPU ìµœì í™”
   - ì¡°ëª… ì¶”ì •

2. ğŸ“… **ì¤‘ê¸° ê°œì„  ê³„íš** (1-2ì¼)
   - MASt3R í†µí•©
   - ControlNet-Depth ì¶”ê°€

3. ğŸ¯ **ì¥ê¸° ë¡œë“œë§µ** (1ì£¼+)
   - NeRF íŒŒì´í”„ë¼ì¸ ì—°êµ¬
   - Production ìµœì í™”
